# automated_crawler.py
"""
å…¨æ–°çš„å…¨è‡ªåŠ¨åŒ–çˆ¬è™«æ§åˆ¶å™¨ - æ”¯æŒå¤šå…¬ä¼—å·
"""
import logging
import time
import os
import json
import pandas as pd

from src.proxy.read_cookie import ReadCookie
from src.crawler.batch_readnum_spider import BatchReadnumSpider
from src.ui.excel_auto_crawler import ExcelAutoCrawler
from src.database.database_manager import DatabaseManager
from src.database.account_status_manager import AccountStatusManager
from src.database.database_config import get_database_config
from src.ui.wechat_browser_automation import WeChatBrowserAutomation, UI_AUTOMATION_AVAILABLE
from config.config_manager import get_crawler_config

class AutomatedCrawler:
    """
    åè°ƒæ•´ä¸ªè‡ªåŠ¨åŒ–æµç¨‹çš„æ§åˆ¶å™¨ - æ”¯æŒå¤šå…¬ä¼—å·:
    1. ä»Excelè¯»å–æ‰€æœ‰å…¬ä¼—å·é“¾æ¥
    2. å¯¹æ¯ä¸ªå…¬ä¼—å·æ‰§è¡Œå®Œæ•´çš„æŠ“å–æµç¨‹:
       - å¯åŠ¨ mitmproxy æŠ“å–å™¨ (ä¼šè‡ªåŠ¨è®¾ç½®ä»£ç†)
       - è¿è¡Œ UI è‡ªåŠ¨åŒ–æ‰“å¼€å¾®ä¿¡æ–‡ç« ä»¥è§¦å‘æŠ“å–
       - ç­‰å¾…å¹¶éªŒè¯ Cookie æ˜¯å¦æˆåŠŸæŠ“å–
       - åœæ­¢ mitmproxy æŠ“å–å™¨ (ä¼šè‡ªåŠ¨å…³é—­ä»£ç†)
       - ä½¿ç”¨è·å–åˆ°çš„ Cookie è¿è¡Œæ‰¹é‡çˆ¬è™«
    3. æ±‡æ€»æ‰€æœ‰å…¬ä¼—å·çš„æŠ“å–ç»“æœ
    """
    def __init__(self, excel_path="target_articles.xlsx", save_to_db=True, db_config=None, crawler_config=None):
        self.logger = logging.getLogger()
        # è‹¥æœªæ˜¾å¼ä¼ å…¥ excel_path åˆ™ä½¿ç”¨é…ç½®ä¸­çš„ excel_file
        cfg_excel = (crawler_config or get_crawler_config()).get('excel_file', 'target_articles.xlsx')
        self.excel_path = excel_path if excel_path != "target_articles.xlsx" else cfg_excel
        # é…ç½®
        self.crawler_config = crawler_config or get_crawler_config()
        self.cookie_wait_timeout = self.crawler_config.get('cookie_wait_timeout', 120)
        self.account_delay = self.crawler_config.get('account_delay', 15)
        self.days_back = self.crawler_config.get('days_back', 90)
        self.max_pages = self.crawler_config.get('max_pages', 200)
        self.articles_per_page = self.crawler_config.get('articles_per_page', 5)
        
        # çª—å£ç®¡ç†é…ç½®
        ui_config = self.crawler_config.get('ui_automation', {})
        self.auto_close_browser_windows = ui_config.get('auto_close_browser_windows', True)
        self.close_windows_between_accounts = ui_config.get('close_windows_between_accounts', True)
        # æ•°æ®åº“
        self.save_to_db = save_to_db
        self.db_config = db_config or get_database_config()
        self.db_manager = None
        self.account_status_manager = None
        if self.save_to_db:
            try:
                self.db_manager = DatabaseManager(**self.db_config)
                self.account_status_manager = AccountStatusManager(self.db_manager)
                count = self.db_manager.get_articles_count()
                self.logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸï¼å½“å‰æœ‰ {count} ç¯‡æ–‡ç« ")
            except Exception as e:
                self.logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                self.logger.warning("âš ï¸ å°†åªä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“")
                self.save_to_db = False

    def _get_all_target_urls_from_excel(self) -> list:
        """
        ä»Excelæ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰æœ‰æ•ˆçš„å…¬ä¼—å·é“¾æ¥
        :return: åŒ…å«æ‰€æœ‰æœ‰æ•ˆé“¾æ¥å’Œå…¬ä¼—å·åç§°çš„åˆ—è¡¨
        """
        self.logger.info(f"æ­£åœ¨ä» {self.excel_path} è¯»å–æ‰€æœ‰ç›®æ ‡URL...")
        if not os.path.exists(self.excel_path):
            self.logger.error(f"Excelæ–‡ä»¶æœªæ‰¾åˆ°: {self.excel_path}")
            return []

        try:
            df = pd.read_excel(self.excel_path)
            url_column = 'æ–‡ç« é“¾æ¥' if 'æ–‡ç« é“¾æ¥' in df.columns else 'url'
            name_column = 'å…¬ä¼—å·åç§°' if 'å…¬ä¼—å·åç§°' in df.columns else 'name'

            if url_column not in df.columns:
                self.logger.error("Excelä¸­æœªæ‰¾åˆ° 'æ–‡ç« é“¾æ¥' æˆ– 'url' åˆ—ã€‚")
                return []

            valid_targets = []
            for index, row in df.iterrows():
                url = row[url_column]
                name = row.get(name_column, f"å…¬ä¼—å·_{index+1}") if name_column in df.columns else f"å…¬ä¼—å·_{index+1}"

                if pd.notna(url) and 'mp.weixin.qq.com' in str(url):
                    valid_targets.append({
                        'name': str(name),
                        'url': str(url),
                        'index': index + 1
                    })
                    self.logger.info(f"æ‰¾åˆ°æœ‰æ•ˆç›®æ ‡ {index+1}: {name} - {str(url)[:50]}...")

            self.logger.info(f"å…±æ‰¾åˆ° {len(valid_targets)} ä¸ªæœ‰æ•ˆçš„å…¬ä¼—å·ç›®æ ‡")
            return valid_targets

        except Exception as e:
            self.logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
            return []

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„å¤šå…¬ä¼—å·è‡ªåŠ¨åŒ–æµç¨‹"""
        self.logger.info("="*80)
        self.logger.info("ğŸš€ å¤šå…¬ä¼—å·å…¨æ–°è‡ªåŠ¨åŒ–æµç¨‹å¯åŠ¨ ğŸš€")
        self.logger.info("="*80)

        # è·å–æ‰€æœ‰ç›®æ ‡å…¬ä¼—å·
        all_targets = self._get_all_target_urls_from_excel()
        if not all_targets:
            self.logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å…¬ä¼—å·é“¾æ¥ï¼Œæµç¨‹ä¸­æ­¢ã€‚")
            return False

        # åˆå§‹åŒ–æ‰€æœ‰å…¬ä¼—å·çš„çŠ¶æ€
        if self.account_status_manager:
            for target in all_targets:
                # ä½¿ç”¨URLä½œä¸ºaccount_idï¼Œå…¬ä¼—å·åç§°ä½œä¸ºaccount_name
                self.account_status_manager.initialize_account_status(target['url'], target['name'])

        self.logger.info(f"ğŸ“‹ å…±æ‰¾åˆ° {len(all_targets)} ä¸ªå…¬ä¼—å·ï¼Œå¼€å§‹é€ä¸ªå¤„ç†...")

        # ç”¨äºå­˜å‚¨æ‰€æœ‰å…¬ä¼—å·çš„æŠ“å–ç»“æœ
        all_results = []
        successful_count = 0
        failed_count = 0

        try:
            for i, target in enumerate(all_targets, 1):
                self.logger.info("="*60)
                self.logger.info(f"ğŸ“ å¤„ç†ç¬¬ {i}/{len(all_targets)} ä¸ªå…¬ä¼—å·: {target['name']}")
                self.logger.info("="*60)

                # åœ¨å¤„ç†æ–°å…¬ä¼—å·å‰ï¼Œå…³é—­ä¹‹å‰æ‰“å¼€çš„å¾®ä¿¡å†…ç½®æµè§ˆå™¨çª—å£
                # ä»ç¬¬äºŒä¸ªå…¬ä¼—å·å¼€å§‹æ‰§è¡Œå…³é—­æ“ä½œ
                if i > 1 and self.close_windows_between_accounts:
                    self.logger.info(f"[é¢„å¤„ç†] å…³é—­ä¹‹å‰æ‰“å¼€çš„å¾®ä¿¡æµè§ˆå™¨çª—å£ä»¥é˜²æ­¢çª—å£ç´¯ç§¯...")
                    try:
                        # åˆ›å»ºä¸´æ—¶çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å®ä¾‹æ¥å…³é—­çª—å£
                        temp_automation = WeChatBrowserAutomation()
                        temp_automation.close_wechat_browser_windows(keep_main_window=True)
                        self.logger.info("âœ… æµè§ˆå™¨çª—å£æ¸…ç†å®Œæˆ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ æ¸…ç†æµè§ˆå™¨çª—å£æ—¶å‡ºç°è­¦å‘Š: {e}")
                        # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å¤„ç†

                # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºPROCESSING
                if self.account_status_manager:
                    self.account_status_manager.update_account_status(target['url'], 'PROCESSING')

                # ä¸ºæ¯ä¸ªå…¬ä¼—å·åˆ›å»ºç‹¬ç«‹çš„CookieæŠ“å–å™¨
                cookie_reader = None
                auth_info = None  # ç»Ÿä¸€åœ¨å¤–å±‚å£°æ˜ï¼Œä¾¿äºåç»­æ­¥éª¤ä½¿ç”¨
                try:
                    # é¢„æ£€æŸ¥ï¼šå°è¯•å¤ç”¨æœ¬åœ°Cookieï¼Œè‹¥æœ‰æ•ˆåˆ™è·³è¿‡UIæŠ“åŒ…
                    try:
                        self.logger.info("[é¢„æ£€æŸ¥] å°è¯•å¤ç”¨æœ¬åœ° wechat_keys.txt ä¸­çš„Cookie...")
                        reuse_reader = ReadCookie(delete_existing_file=False)
                        auth_info = reuse_reader.get_latest_cookies()
                        if auth_info:
                            # æ ¡éªŒbizæ˜¯å¦åŒ¹é…å½“å‰ç›®æ ‡
                            import re
                            m = re.search(r"__biz=([^&]+)", target['url'])
                            target_biz = m.group(1) if m else None
                            if target_biz and target_biz != auth_info.get('biz'):
                                self.logger.info(f"æœ¬åœ°Cookieçš„biz({auth_info.get('biz')})ä¸ç›®æ ‡biz({target_biz})ä¸åŒ¹é…ï¼Œæ”¾å¼ƒå¤ç”¨")
                                auth_info = None
                        if auth_info:
                            # å¿«é€Ÿæ ¡éªŒCookieæœ‰æ•ˆæ€§
                            self.logger.info("æ£€æµ‹åˆ°å¯ç”¨Cookieï¼Œå…ˆè¡Œæ ¡éªŒæœ‰æ•ˆæ€§...")
                            try:
                                test_spider = BatchReadnumSpider(
                                    auth_info=auth_info,
                                    save_to_db=self.save_to_db,
                                    db_config=self.db_config,
                                    unit_name=""  # ä¸é¢„è®¾å•ä½åç§°ï¼Œè®©çˆ¬è™«è‡ªåŠ¨æ ¹æ®å…¬ä¼—å·åç§°æ˜ å°„
                                )
                                if test_spider.validate_cookie():
                                    self.logger.info("âœ… æœ¬åœ°CookieéªŒè¯é€šè¿‡ï¼Œå°†ç›´æ¥ä½¿ç”¨è¯¥Cookieè¿›è¡Œçˆ¬å–ï¼ˆè·³è¿‡UIæŠ“åŒ…ï¼‰")
                                else:
                                    self.logger.info("æœ¬åœ°CookieéªŒè¯å¤±è´¥ï¼Œè¿›å…¥UIè‡ªåŠ¨åŒ–æŠ“åŒ…æµç¨‹")
                                    auth_info = None
                            except Exception as ve:
                                self.logger.info(f"æœ¬åœ°CookieéªŒè¯å‡ºç°å¼‚å¸¸ï¼Œè¿›å…¥UIè‡ªåŠ¨åŒ–æŠ“åŒ…æµç¨‹: {ve}")
                                auth_info = None
                    except Exception as pre_e:
                        self.logger.debug(f"é¢„æ£€æŸ¥å¤ç”¨Cookieæ—¶å‡ºç°é—®é¢˜: {pre_e}")

                    # æ­¥éª¤1-4: è‹¥æœªå¤ç”¨Cookieï¼Œåˆ™åˆ›å»ºæŠ“å–å™¨å¹¶æ‰§è¡ŒUIæŠ“åŒ…
                    if not auth_info:
                        self.logger.info(f"[æ­¥éª¤ 1/5] ä¸º '{target['name']}' åˆ›å»ºç‹¬ç«‹çš„ Cookie æŠ“å–å™¨...")
                        cookie_reader = ReadCookie()  # æ¯ä¸ªå…¬ä¼—å·ç‹¬ç«‹åˆ›å»ºï¼Œä¼šåˆ é™¤æ—§æ–‡ä»¶
                        if not cookie_reader.start_cookie_extractor():
                            self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' Cookie æŠ“å–å™¨å¯åŠ¨å¤±è´¥ï¼Œè·³è¿‡æ­¤å…¬ä¼—å·")
                            failed_count += 1
                            # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                            if self.account_status_manager:
                                self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', "CookieæŠ“å–å™¨å¯åŠ¨å¤±è´¥")
                                self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' CookieæŠ“å–å™¨å¯åŠ¨å¤±è´¥")
                            continue
                        self.logger.info("âœ… Cookie æŠ“å–å™¨å·²åœ¨åå°è¿è¡Œã€‚")

                        # æ­¥éª¤2: è¿è¡Œ UI è‡ªåŠ¨åŒ–è§¦å‘æŠ“å–
                        self.logger.info(f"[æ­¥éª¤ 2/5] ä¸º '{target['name']}' å¯åŠ¨ UI è‡ªåŠ¨åŒ–...")
                        try:
                            ui_crawler = ExcelAutoCrawler()
                            # ç›´æ¥ä¼ é€’å½“å‰å…¬ä¼—å·çš„URLï¼Œå¹¶ä¼ é€’cookie_readerä»¥å¯ç”¨æ™ºèƒ½åˆ·æ–°åœæ­¢
                            success = ui_crawler.automation.send_and_open_latest_link(target['url'], cookie_reader=cookie_reader)
                            if not success:
                                self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' UI è‡ªåŠ¨åŒ–è§¦å‘å¤±è´¥ï¼Œè·³è¿‡æ­¤å…¬ä¼—å·")
                                cookie_reader.stop_cookie_extractor()
                                failed_count += 1
                                # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                                if self.account_status_manager:
                                    self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', "UIè‡ªåŠ¨åŒ–è§¦å‘å¤±è´¥")
                                    self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' UIè‡ªåŠ¨åŒ–è§¦å‘å¤±è´¥")
                                continue
                        except Exception as e:
                            self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' UI è‡ªåŠ¨åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                            cookie_reader.stop_cookie_extractor()
                            failed_count += 1
                            # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                            if self.account_status_manager:
                                self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', f"UIè‡ªåŠ¨åŒ–å¼‚å¸¸: {str(e)}")
                                self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' UIè‡ªåŠ¨åŒ–å¼‚å¸¸: {str(e)}")
                            continue
                        self.logger.info("âœ… UI è‡ªåŠ¨åŒ–å·²æˆåŠŸè§¦å‘é“¾æ¥æ‰“å¼€ã€‚")

                        # æ­¥éª¤3: ç­‰å¾…å¹¶éªŒè¯ Cookie
                        self.logger.info(f"[æ­¥éª¤ 3/5] ç­‰å¾… '{target['name']}' çš„ Cookie æ•°æ®...")
                        if not cookie_reader.wait_for_new_cookie(timeout=self.cookie_wait_timeout):
                            self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' ç­‰å¾… Cookie è¶…æ—¶ï¼Œè·³è¿‡æ­¤å…¬ä¼—å·")
                            cookie_reader.stop_cookie_extractor()
                            failed_count += 1
                            # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                            if self.account_status_manager:
                                self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', "ç­‰å¾…Cookieè¶…æ—¶")
                                self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' ç­‰å¾…Cookieè¶…æ—¶")
                            continue

                        # è·å–å¹¶éªŒè¯cookieæ˜¯å¦æœ‰æ•ˆ
                        auth_info = cookie_reader.get_latest_cookies()
                        if not auth_info:
                            self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' Cookie è§£æå¤±è´¥")
                            self.logger.error("ğŸ’¡ å¯èƒ½çš„åŸå› :")
                            self.logger.error("   1. mitmproxy æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°å¾®ä¿¡è¯·æ±‚")
                            self.logger.error("   2. å¾®ä¿¡å†…ç½®æµè§ˆå™¨æ²¡æœ‰æ­£ç¡®æ‰“å¼€é“¾æ¥")
                            self.logger.error("   3. ç½‘ç»œè¿æ¥é—®é¢˜æˆ–ä»£ç†è®¾ç½®é—®é¢˜")
                            self.logger.error("ğŸ’¡ å»ºè®®:")
                            self.logger.error("   1. æ£€æŸ¥å¾®ä¿¡æ˜¯å¦æ­£å¸¸æ‰“å¼€äº†æ–‡ç« é“¾æ¥")
                            self.logger.error("   2. æ‰‹åŠ¨åœ¨å¾®ä¿¡ä¸­åˆ·æ–°æ–‡ç« é¡µé¢")
                            self.logger.error("   3. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
                            cookie_reader.stop_cookie_extractor()
                            failed_count += 1
                            # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                            if self.account_status_manager:
                                self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', "Cookieè§£æå¤±è´¥")
                                self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' Cookieè§£æå¤±è´¥")
                            continue
                        self.logger.info("âœ… æˆåŠŸè·å–å¹¶éªŒè¯äº†æ–°çš„ Cookieã€‚")

                        # æ­¥éª¤4: åœæ­¢ mitmproxy æŠ“å–å™¨
                        self.logger.info(f"[æ­¥éª¤ 4/5] åœæ­¢ '{target['name']}' çš„ Cookie æŠ“å–å™¨...")
                        cookie_reader.stop_cookie_extractor()
                        time.sleep(3)  # ç­‰å¾…ä»£ç†å®Œå…¨å…³é—­
                        self.logger.info("âœ… Cookie æŠ“å–å™¨å·²åœæ­¢ï¼Œç³»ç»Ÿä»£ç†å·²æ¢å¤ã€‚")

                    # æ­¥éª¤5: è¿è¡Œæ‰¹é‡çˆ¬è™«ï¼ˆå¸¦Cookieé‡æ–°æŠ“å–æœºåˆ¶ï¼‰
                    self.logger.info(f"[æ­¥éª¤ 5/5] å¼€å§‹çˆ¬å– '{target['name']}' çš„æ–‡ç« ...")

                    max_attempts = 2  # æœ€å¤šå°è¯•2æ¬¡ï¼ˆç¬¬ä¸€æ¬¡å¤±è´¥åé‡æ–°æŠ“å–Cookieå†è¯•ä¸€æ¬¡ï¼‰
                    batch_spider = None

                    for attempt in range(max_attempts):
                        try:
                            self.logger.info(f"ğŸ”„ ç¬¬ {attempt + 1}/{max_attempts} æ¬¡å°è¯•çˆ¬å–...")
                            batch_spider = BatchReadnumSpider(
                                auth_info=auth_info,
                                save_to_db=self.save_to_db,
                                db_config=self.db_config,
                                unit_name=""  # ä¸é¢„è®¾å•ä½åç§°ï¼Œè®©çˆ¬è™«è‡ªåŠ¨æ ¹æ®å…¬ä¼—å·åç§°æ˜ å°„
                            )

                            # å…ˆéªŒè¯Cookie
                            if not batch_spider.validate_cookie():
                                if attempt < max_attempts - 1:
                                    self.logger.warning("âš ï¸ CookieéªŒè¯å¤±è´¥ï¼ˆret=-3ï¼‰ï¼Œå‡†å¤‡ä»…åˆ·æ–°æ–‡ç« é¡µé¢ä»¥é‡æ–°æŠ“åŒ…...")

                                    # é‡æ–°æŠ“å–Cookieï¼ˆä»…å¯åŠ¨æŠ“å–å™¨ï¼Œä¸é‡å¤ç²˜è´´ç‚¹å‡»ï¼‰
                                    self.logger.info("ğŸ”„ é‡æ–°å¯åŠ¨CookieæŠ“å–å™¨...")
                                    fresh_cookie_reader = ReadCookie()
                                    if not fresh_cookie_reader.start_cookie_extractor():
                                        self.logger.error("âŒ é‡æ–°å¯åŠ¨CookieæŠ“å–å™¨å¤±è´¥")
                                        break

                                    # ä»…åˆ·æ–°å½“å‰æ–‡ç« é¡µé¢
                                    try:
                                        if not UI_AUTOMATION_AVAILABLE:
                                            self.logger.error("âŒ UIè‡ªåŠ¨åŒ–ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œåˆ·æ–°")
                                        else:
                                            self.logger.info("ğŸ” ä¸é‡æ–°ç²˜è´´é“¾æ¥ï¼Œç›´æ¥åˆ·æ–°å·²æ‰“å¼€çš„æ–‡ç« é¡µé¢ä»¥è§¦å‘æ–°è¯·æ±‚â€¦")
                                            refresher = WeChatBrowserAutomation()
                                            # åˆ·æ–°æ¬¡æ•°é€‚å½“å¢åŠ ï¼Œæé«˜è§¦å‘æ¦‚ç‡
                                            refresher.auto_refresh_browser(refresh_count=self.crawler_config.get('refresh_count', 3),
                                                                           refresh_delay=self.crawler_config.get('refresh_delay', 3.0),
                                                                           cookie_reader=fresh_cookie_reader)
                                    except Exception as e:
                                        self.logger.warning(f"åˆ·æ–°æ–‡ç« é¡µé¢æ—¶å‡ºé”™: {e}")

                                    # ç­‰å¾…æ–°Cookie
                                    if not fresh_cookie_reader.wait_for_new_cookie(timeout=self.cookie_wait_timeout):
                                        self.logger.error("âŒ é‡æ–°ç­‰å¾…Cookieè¶…æ—¶")
                                        fresh_cookie_reader.stop_cookie_extractor()
                                        break

                                    # è·å–æ–°çš„è®¤è¯ä¿¡æ¯
                                    auth_info = fresh_cookie_reader.get_latest_cookies()
                                    fresh_cookie_reader.stop_cookie_extractor()
                                    time.sleep(3)

                                    if not auth_info:
                                        self.logger.error("âŒ é‡æ–°è·å–Cookieå¤±è´¥")
                                        break

                                    self.logger.info("âœ… æˆåŠŸé€šè¿‡åˆ·æ–°é‡æ–°è·å–Cookieï¼Œç»§ç»­å°è¯•...")
                                    continue
                                else:
                                    self.logger.error("âŒ å¤šæ¬¡å°è¯•åCookieä»ç„¶æ— æ•ˆ")
                                    # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                                    if self.account_status_manager:
                                        self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', f"å¤šæ¬¡å°è¯•åCookieä»ç„¶æ— æ•ˆ")
                                        self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' å¤šæ¬¡å°è¯•åCookieä»ç„¶æ— æ•ˆ")
                                    break

                            # Cookieæœ‰æ•ˆï¼Œå¼€å§‹æ­£å¼çˆ¬å–
                            self.logger.info("âœ… CookieéªŒè¯æˆåŠŸï¼Œå¼€å§‹æ­£å¼çˆ¬å–...")
                            
                            batch_spider.batch_crawl_readnum(
                                max_pages=self.max_pages,
                                articles_per_page=self.articles_per_page,
                                days_back=self.days_back
                            )
                            break  # æˆåŠŸå®Œæˆï¼Œè·³å‡ºé‡è¯•å¾ªç¯

                        except Exception as e:
                            self.logger.error(f"âŒ ç¬¬ {attempt + 1} æ¬¡å°è¯•æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                            if attempt < max_attempts - 1:
                                self.logger.info("ğŸ”„ å‡†å¤‡é‡è¯•...")
                                time.sleep(5)
                            else:
                                self.logger.error("âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†")
                                # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                                if self.account_status_manager:
                                    self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', f"æ‰€æœ‰å°è¯•éƒ½å¤±è´¥: {str(e)}")
                                    self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' æ‰€æœ‰å°è¯•éƒ½å¤±è´¥: {str(e)}")

                    if not batch_spider or not batch_spider.articles_data:
                        self.logger.error(f"âŒ å…¬ä¼—å· '{target['name']}' çˆ¬å–å¤±è´¥")
                        failed_count += 1
                        # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                        if self.account_status_manager:
                            self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', f"çˆ¬å–å¤±è´¥")
                            self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' çˆ¬å–å¤±è´¥")
                        continue

                    if batch_spider.articles_data:
                        # ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ å…¬ä¼—å·ä¿¡æ¯
                        for article in batch_spider.articles_data:
                            article['å…¬ä¼—å·åç§°'] = target['name']
                            article['å…¬ä¼—å·åºå·'] = i

                        all_results.extend(batch_spider.articles_data)

                        # ä¿å­˜å½“å‰å…¬ä¼—å·çš„æ•°æ®
                        timestamp = time.strftime("%Y%m%d_%H%M%S")
                        excel_file = batch_spider.save_to_excel(f"./data/readnum_batch/readnum_{target['name']}_{timestamp}.xlsx")
                        json_file = batch_spider.save_to_json(f"./data/readnum_batch/readnum_{target['name']}_{timestamp}.json")

                        self.logger.info(f"âœ… å…¬ä¼—å· '{target['name']}' çˆ¬å–å®Œæˆï¼è·å– {len(batch_spider.articles_data)} ç¯‡æ–‡ç« ")
                        self.logger.info(f"ğŸ“Š æ•°æ®å·²ä¿å­˜åˆ°: {excel_file}")
                        successful_count += 1

                        # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºCOMPLETED
                        if self.account_status_manager:
                            self.account_status_manager.update_account_status(target['url'], 'COMPLETED')
                    else:
                        self.logger.warning(f"âš ï¸ å…¬ä¼—å· '{target['name']}' æœªè·å–åˆ°ä»»ä½•æ–‡ç« æ•°æ®")
                        failed_count += 1

                        # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                        if self.account_status_manager:
                            self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', 'æœªè·å–åˆ°ä»»ä½•æ–‡ç« æ•°æ®')
                            self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' æœªè·å–åˆ°ä»»ä½•æ–‡ç« æ•°æ®")

                    # å…¬ä¼—å·é—´å»¶è¿Ÿï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                    if i < len(all_targets):
                        self.logger.info(f"â³ å…¬ä¼—å·é—´å»¶è¿Ÿ {self.account_delay} ç§’...")
                        time.sleep(self.account_delay)
                        
                        # åœ¨å»¶è¿ŸæœŸé—´ä¹Ÿå…³é—­æµè§ˆå™¨çª—å£ï¼Œç¡®ä¿ä¸‹ä¸€ä¸ªå…¬ä¼—å·å¼€å§‹æ—¶çª—å£å¹²å‡€
                        if self.auto_close_browser_windows:
                            try:
                                temp_automation = WeChatBrowserAutomation()
                                temp_automation.close_wechat_browser_windows(keep_main_window=True)
                            except Exception as e:
                                self.logger.debug(f"å»¶è¿ŸæœŸé—´æ¸…ç†çª—å£æ—¶å‡ºç°é—®é¢˜: {e}")

                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†å…¬ä¼—å· '{target['name']}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # ç¡®ä¿åœæ­¢æŠ“å–å™¨
                    if cookie_reader:
                        try:
                            cookie_reader.stop_cookie_extractor()
                        except:
                            pass
                    failed_count += 1

                    # æ›´æ–°å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                    if self.account_status_manager:
                        self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', str(e))
                        self.account_status_manager.record_crawl_exception(f"å…¬ä¼—å· '{target['name']}' å¤„ç†å¼‚å¸¸: {str(e)}")
                    continue

        except Exception as e:
            self.logger.error(f"âŒ è‡ªåŠ¨åŒ–æµç¨‹å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

            # æ›´æ–°æ‰€æœ‰å…¬ä¼—å·çŠ¶æ€ä¸ºEXCEPTIONå¹¶è®°å½•åˆ°å¼‚å¸¸è¡¥å¿è¡¨
            if self.account_status_manager:
                for target in all_targets:
                    self.account_status_manager.update_account_status(target['url'], 'EXCEPTION', f"å…¨å±€å¼‚å¸¸: {str(e)}")
                # è®°å½•å…¨å±€å¼‚å¸¸åˆ°å¼‚å¸¸è¡¥å¿è¡¨
                self.account_status_manager.record_crawl_exception(f"å…¨å±€å¼‚å¸¸: {str(e)}")

            return False

        # æ±‡æ€»ç»“æœ
        self.logger.info("="*80)
        self.logger.info("ğŸ“Š å¤šå…¬ä¼—å·çˆ¬å–æ±‡æ€»ç»“æœ")
        self.logger.info("="*80)
        self.logger.info(f"âœ… æˆåŠŸå¤„ç†: {successful_count} ä¸ªå…¬ä¼—å·")
        self.logger.info(f"âŒ å¤±è´¥å¤„ç†: {failed_count} ä¸ªå…¬ä¼—å·")
        self.logger.info(f"ğŸ“„ æ€»è®¡æ–‡ç« : {len(all_results)} ç¯‡")

        # æ±‡æ€»æ•°æ®ä¿å­˜åŠŸèƒ½å·²ç§»é™¤

        self.logger.info("="*80)
        self.logger.info("âœ… å¤šå…¬ä¼—å·å…¨æ–°è‡ªåŠ¨åŒ–æµç¨‹æ‰§è¡Œå®Œæ¯• âœ…")
        self.logger.info("="*80)

        # å¦‚æœæµç¨‹æˆåŠŸå®Œæˆï¼Œæ¸…é™¤å¼‚å¸¸è®°å½•
        if self.account_status_manager and successful_count > 0:
            self.account_status_manager.clear_crawl_exception()

        return successful_count > 0  # åªè¦æœ‰ä¸€ä¸ªæˆåŠŸå°±ç®—æˆåŠŸ
